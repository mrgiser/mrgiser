<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[redis笔记：对象]]></title>
    <url>%2F2018%2F05%2F25%2Fredisobject%2F</url>
    <content type="text"><![CDATA[对象的类型redis中的对象包括： 字符串对象 REDIS_STRING 列表对象 REDIS_LIST 哈希对象 REDIS_HASH 集合对象 REDIS_SET 有序集合对象 REDIS_ZSET 对象的底层编码实现 REDIS_ENCODING_INT long类型的整数 REDIS_ENCODING_EMBSTR embstr编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 类型与编码的对应关系 类型 编码方式 说明 REDIS_STRING REDIS_ENCODING_INT 整数值实现的字符串 REDIS_STRING REDIS_ENCODING_EMBSTR embstr编码的字符串 REDIS_STRING REDIS_ENCODING_RAW 简单动态字符串的字符串 REDIS_LIST REDIS_ENCODING_ZIPLIST 压缩列表实现的列表 REDIS_LIST REDIS_ENCODING_LINKEDLIST 双端列表实现的列表 REDIS_HASH REDIS_ENCODING_HT 字典实现的哈希对象 REDIS_HASH REDIS_ENCODING_ZIPLIST 压缩列表实现的哈希对象 REDIS_SET REDIS_ENCODING_INTSET 整数集合实现的集合 REDIS_SET REDIS_ENCODING_HT 字典实现的集合 REDIS_ZSET REDIS_ENCODING_SKIPLIST 跳跃表实现的有序集合 REDIS_ZSET REDIS_ENCODING_ZIPLIST 压缩列表实现的有序集合 字符串对象 是整数、并可以用long表示，编码为 REDIS_ENCODING_INT 是一个字符串值，长度大于32字节，REDIS_ENCODING_RAW 是一个字符串值，长度小于等于32， REDIS_ENCODING_EMBSTR（减少内存的申请，释放操作） 要点： EMBSTR通过一次申请连续的内存空间存储redisObject,sdshdr 释放也只需要一次 long double采用字符串存储 EMBSTR修改直接转换成raw，EMBSTR可以理解为只读 列表对象使用压缩列表默认条件（可修改）： 所保存的字符串元素长度都小于64字节 元素数量小于512个 不满足上述条件的采用双端链表实现列表对象 使用过程中不满足1.2条时，会转化底层的实现为双端 哈希对象使用压缩列表时： 先将保存键的节点放在压缩列表的队尾，然后是保存了值的节点 所以同一键值对的节点是在一起的，键在前，值在后 先添加的在压缩列表的头部，后来的在尾部 使用字典实现哈希： 哈希对象的每个键值使用一个字典的键值表示 使用压缩列表的默认条件（可修改）： 所有的键值对的键、值都小于64字节 键值对数量小于512 集合对象可以采用整数结合、字典实现。采用字典实现时，字典每个键都是一个集合元素，字典的值设置为null使用整数集合的默认条件（可以修改）： 所有元素都是整数 元素数量不超多512 有序集合可以使用压缩列表、跳跃表和字典实现。 使用压缩列表：每个元素采用两个紧挨一起的节点表示，前一个表示成员，后一个表示分数。压缩列表内部按照分数大小前后排序。 使用跳跃表和字典：同时包含一个跳跃表和字典。跳跃表按照分数大小保存了所有的集合元素（包括成员、分数），字典实现了成员到分值的映射，字典的键为元素成员，值为元素的分数，能够很快找到成员的分数 通过跳跃表实现范围类型的操作。 字典实现快速查找分数 字典和跳跃表通过指针共享相同的元素的成员和分数，不会冗余 使用压缩列表的默认条件（可以修改）： 元素数量小于128 元素的成员的长度都小于64字节 引用计数 redisObject结构中的refcount属性记录了对象的引用计数 object refcount key 查看引用计数 对象共享 默认初始化服务的时候新建0-9999的整数字符串进行共享 只共享整数因为对比消耗cpu少 对象空转时间 redisObject结构中的lru记录最后一次使用时间 object idletime key 查看空转时间 idletime不会更新lru]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>学习笔记</tag>
        <tag>内存数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis笔记：压缩列表]]></title>
    <url>%2F2018%2F05%2F15%2Fredisziplist%2F</url>
    <content type="text"><![CDATA[概述 压缩列表是列表、哈希的底层实现之一 当列表只包含少量列表项，并且要么是小整数值、短字符串时，采用压缩列表 哈希表只包含少量键值对，键值对的键和值要么都是小整数、短字符串，采用压缩列表实现哈希 123456789101112131415161718192021&lt;zlbytes&gt; 是一个无符号整数，保存着 ziplist 使用的内存数量。通过这个值，程序可以直接对 ziplist 的内存大小进行调整，而无须为了计算 ziplist 的内存大小而遍历整个列表。&lt;zltail&gt; is the offset to the last entry in the list. This allows a popoperation on the far side of the list without the need for full traversal.&lt;zltail&gt; 保存着到达列表中最后一个节点的偏移量。这个偏移量使得对表尾的 pop 操作可以在无须遍历整个列表的情况下进行。&lt;zllen&gt; is the number of entries.When this value is larger than 2**16-2,we need to traverse the entire list to know how many items it holds.&lt;zllen&gt; 保存着列表中的节点数量。当 zllen 保存的值大于 2**16-2 时，程序需要遍历整个列表才能知道列表实际包含了多少个节点。&lt;zlend&gt; is a single byte special value, equal to 255, which indicates theend of the list.&lt;zlend&gt; 的长度为 1 字节，值为 255 ，标识列表的末尾。 每个 ziplist 节点的前面都带有一个 header ，这个 header 包含两部分信息： 前置节点的长度，在程序从后向前遍历时使用。 当前节点所保存的值的类型和长度。 编码前置节点的长度的方法如下： 如果前置节点的长度小于 254 字节，那么程序将使用 1 个字节来保存这个长度值。 如果前置节点的长度大于等于 254 字节，那么程序将使用 5 个字节来保存这个长度值： 第 1 个字节的值将被设为 254 ，用于标识这是一个 5 字节长的长度值。 之后的 4 个字节则用于保存前置节点的实际长度。 header 另一部分的内容和节点所保存的值有关。 如果节点保存的是字符串值，那么这部分 header 的头 2 个位将保存编码字符串长度所使用的类型，而之后跟着的内容则是字符串的实际长度。 |00pppppp| - 1 byte String value with length less than or equal to 63 bytes (6 bits). 字符串的长度小于或等于 63 字节。 |01pppppp|qqqqqqqq| - 2 bytes String value with length less than or equal to 16383 bytes (14 bits). 字符串的长度小于或等于 16383 字节。 |10__|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| - 5 bytes String value with length greater than or equal to 16384 bytes. 字符串的长度大于或等于 16384 字节。 如果节点保存的是整数值，那么这部分 header 的头 2 位都将被设置为 1 ，而之后跟着的 2 位则用于标识节点所保存的整数的类型。 |11000000| - 1 byte Integer encoded as int16_t (2 bytes). 节点的值为 int16_t 类型的整数，长度为 2 字节。 |11010000| - 1 byte Integer encoded as int32_t (4 bytes). 节点的值为 int32_t 类型的整数，长度为 4 字节。 |11100000| - 1 byte Integer encoded as int64_t (8 bytes). 节点的值为 int64_t 类型的整数，长度为 8 字节。 |11110000| - 1 byte Integer encoded as 24 bit signed (3 bytes). 节点的值为 24 位（3 字节）长的整数。 |11111110| - 1 byte Integer encoded as 8 bit signed (1 byte). 节点的值为 8 位（1 字节）长的整数。 |1111xxxx| - (with xxxx between 0000 and 1101) immediate 4 bit integer. Unsigned integer from 0 to 12. The encoded value is actually from 1 to 13 because 0000 and 1111 can not be used, so 1 should be subtracted from the encoded 4 bit value to obtain the right value. 节点的值为介于 0 至 12 之间的无符号整数。 因为 0000 和 1111 都不能使用，所以位的实际值将是 1 至 13 。 程序在取得这 4 个位的值之后，还需要减去 1 ，才能计算出正确的值。 比如说，如果位的值为 0001 = 1 ，那么程序返回的值将是 1 - 1 = 0 。 |11111111| - End of ziplist. ziplist 的结尾标识 所有整数都表示为小端字节序。 ziplist 示例图123456789101112131415161718192021222324252627282930313233/* 空白 ziplist 示例图area |&lt;---- ziplist header ----&gt;|&lt;-- end --&gt;|size 4 bytes 4 bytes 2 bytes 1 byte +---------+--------+-------+-----------+component | zlbytes | zltail | zllen | zlend | | | | | |value | 1011 | 1010 | 0 | 1111 1111 | +---------+--------+-------+-----------+ ^ | ZIPLIST_ENTRY_HEAD &amp;address ZIPLIST_ENTRY_TAIL &amp; ZIPLIST_ENTRY_END非空 ziplist 示例图area |&lt;---- ziplist header ----&gt;|&lt;----------- entries -------------&gt;|&lt;-end-&gt;|size 4 bytes 4 bytes 2 bytes ? ? ? ? 1 byte +---------+--------+-------+--------+--------+--------+--------+-------+component | zlbytes | zltail | zllen | entry1 | entry2 | ... | entryN | zlend | +---------+--------+-------+--------+--------+--------+--------+-------+ ^ ^ ^address | | | ZIPLIST_ENTRY_HEAD | ZIPLIST_ENTRY_END | ZIPLIST_ENTRY_TAIL*/ 连锁更新]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>学习笔记</tag>
        <tag>内存数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis笔记：跳跃表]]></title>
    <url>%2F2018%2F05%2F10%2Frediszskiplist%20%2F</url>
    <content type="text"><![CDATA[通过在每个节点中维持多个只想其他节点的指针，从而达到快速访问节点的目的。 作为redis有序集合键的底层实现之一（数量多，元素字符长） 跳跃表的原理可参考： http://www.cnblogs.com/acfox/p/3688607.html 不同的是：先从上层插入的，而不是底层，先随机判断插入的层数。 redis跳跃表的实现12345678910111213141516171819202122232425262728293031323334353637383940414243/* * 跳跃表节点 */typedef struct zskiplistNode &#123; // 成员对象 robj *obj; // 分值 double score; // 后退指针 struct zskiplistNode *backward; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[];&#125; zskiplistNode;&lt;!--more--&gt;/* * 跳跃表 */typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level;&#125; zskiplist; 创建新跳跃表节点时，根据幂次定律随机生成一个介于1——32之间的值作为level高度的大小。类似与 http://www.cnblogs.com/acfox/p/3688607.html 中的抛硬币 每个节点的高度都是1-32中的随机数 多个节点可以包含相同的分值，但是成员对象必须是唯一的 节点按照分值大小排序，但分值相同时，按照成员对象的大小排序]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>学习笔记</tag>
        <tag>内存数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis笔记：字典]]></title>
    <url>%2F2018%2F05%2F02%2FredisHT%2F</url>
    <content type="text"><![CDATA[字典实现哈希表123456789101112131415161718192021/* * 哈希表 * * 每个字典都使用两个哈希表，从而实现渐进式 rehash 。 */typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used;&#125; dictht; 哈希表节点12345678910111213141516171819/* * 哈希表节点 */typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next;&#125; dictEntry; 字典12345678910111213141516171819202122/* * 字典 */typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 目前正在运行的安全迭代器的数量 int iterators; /* number of iterators currently running */&#125; dict; 字典类型特定函数redis会为用途不同的字典设置不同的类型特点函数123456789101112131415161718192021222324/* * 字典类型特定函数 */typedef struct dictType &#123; // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj);&#125; dictType; 哈希算法12345# 使用字典设置的哈希函数，计算key的哈希值hash = dict-&gt;type-&gt;hashFunction(key);# 使用哈希表的sizemask属性和哈希值，计算出索引值index = hash &amp; dict-&gt;ht[x].sizemask; 建冲突redis采用链地址法解决冲突，新节点添加到链表的表头位置。 rehash 扩展：ht[1]的大小为第一个大于等于ht[0].used*2的2的n次方幂； 收缩：ht[1]的大小为第一个大于等于ht[0].used*2的的2^n； 将ht[0]上的所有键值对rehash到ht[1]上 扩展与收缩的条件： 没有BGSAVE或者BGREWRITEAOF命令，负载因子大于等于1； 有BGSAVE或者BGREWRITEAOF命令时，负载因子大于等于5； 负载因子= ht[0].used / ht[0].size 渐进式rehash渐进式rehash期间，删除，查找，更新等操作会在两个哈希表执行，但是添加操作在ht[1]执行。渐进式rehash将对转移操作平均到对字典的每个添加、删除、查找、更新操作上，避免一次集中操作。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>学习笔记</tag>
        <tag>内存数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Commons pool2 配置详解及其在Jedis中的使用]]></title>
    <url>%2F2018%2F04%2F25%2FCommons%20pool2%20for%20jedis%2F</url>
    <content type="text"><![CDATA[Commons pool2配置参数（以Jedis连接为例） MaxTotal： 最大连接数（空闲+使用中） MaxIdle： 最大空闲连接数 MinIdle： 最小空闲连接数 MaxWaitMillis： 借出连接时最大的等待时间，超时抛错 TimeBetweenEvictionRunsMillis： 后台检测线程周期 MinEvictableIdleTimeMillis：硬闲置时间，连接多久没有使用设置为闲置,检测线程直接剔除闲置，为保持最小空闲数，被剔除后可能重新生成 SoftMinEvictableIdleTimeMillis：软闲置时间，连接多久没有使用设置为闲置,当空闲连接 &gt; MinIdle，才执行剔除闲置，否则维持最小空闲数，即使闲置了也不会剔除，设置MinEvictableIdleTimeMillis后，本参数无效 testOnBorrow：在borrow一个连接时，是否提前进行alidate操作；如果为true，则得到的jedis实例均是可用的； testOnReturn：在连接return给pool时，是否提前进行validate操作； testWhileIdle：如果为true，表示后台线程对idle连接进行扫描，如果validate失败，此连接会被从pool中剔除；只有在timeBetweenEvictionRunsMillis大于0时才有意义； JedisPool验证参数配置用以下代码验证配置参数的作用，其中Ctg开头的是我封装了的，可以使用JedisPool相关类代替123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public static void main(String[] args) &#123; BasicConfigurator.configure(); List&lt;HostAndPort&gt; hostAndPortList = new ArrayList(); //地址列表 HostAndPort host1 = new HostAndPort(&quot;132.122.232.225&quot;,9081); hostAndPortList.add(host1); GenericObjectPoolConfig poolConfig = new JedisPoolConfig(); //线程池配置 poolConfig.setMaxIdle(2); //最大空闲连接数 poolConfig.setMaxTotal(3); // 最大连接数（空闲+使用中） poolConfig.setMinIdle(1); //保持的最小空闲连接数 poolConfig.setMaxWaitMillis(2000); //借出连接时最大的等待时间 poolConfig.setTimeBetweenEvictionRunsMillis(1000); //1秒运行一次检测线程 //硬闲置、软闲置配置 //硬闲置 3秒没有占用设置为闲置, 检测线程直接剔除闲置，保持的最小空闲数，会被剔除且重新生成 poolConfig.setMinEvictableIdleTimeMillis(3000); //软闲置 3秒没有占用设置为闲置, 当空闲连接&gt;最小空闲数，才执行剔除闲置连接，否则维持最小空闲数，即使闲置了也不会剔除 poolConfig.setSoftMinEvictableIdleTimeMillis(3000); CtgJedisPoolConfig config = new CtgJedisPoolConfig(hostAndPortList); config.setDatabase(4970) //分组对应的桶位 .setPassword(&quot;disk#Test1234&quot;) // “用户#密码” .setPoolConfig(poolConfig) //线程池配置 .setPeriod(1000) //后台监控执行周期，毫秒 .setMonitorTimeout(100); //后台监控ping命令超时时间,毫秒 CtgJedisPool pool = new CtgJedisPool(config); //创建连接池 ProxyJedis jedis_1 = null; ProxyJedis jedis_2 = null; ProxyJedis jedis_3 = null; //最大连接为3，以下三次获取能成功 try &#123; logger.info(&quot;=================== get jedis_1 =========================&quot;); jedis_1 = pool.getResource(); &#125; catch (CtgJedisPoolException e) &#123; logger.error(&quot;jedis_1 : Get resource error&quot;); &#125; try &#123; logger.info(&quot;=================== get jedis_2 =========================&quot;); jedis_2 = pool.getResource(); &#125; catch (CtgJedisPoolException e) &#123; logger.error(&quot;jedis_2 : Get resource error&quot;); &#125; try &#123; logger.info(&quot;=================== get jedis_3 =========================&quot;); jedis_3 = pool.getResource(); &#125; catch (CtgJedisPoolException e) &#123; logger.error(&quot;jedis_3 : Get resource error&quot;); &#125; //第四次获取连接，失败。获取连接超时时间为2秒，重试一次，正确结果应该为4秒 ProxyJedis jedis_4 = null; long startTime = 0L; try &#123; logger.info(&quot;=================== get jedis_4 =========================&quot;); startTime = System.currentTimeMillis(); jedis_4 = pool.getResource(); logger.info(&quot;Get resource cost time : &quot; + (System.currentTimeMillis() - startTime)); &#125; catch (CtgJedisPoolException e) &#123; logger.info(&quot;Get resource cost time : &quot; + (System.currentTimeMillis() - startTime)); logger.error(&quot;jedis_4 : Get resource error&quot;); &#125; // 归还一个连接，空闲连接数：1 活跃连接数：2， // 等待后台执行剔除空闲连接，当前空闲连接数 = 需要保持最小空闲连接数，不会进行剔除， // 硬闲置情况下达到限制时间，会被剔除 try&#123; logger.info(&quot;=================== close jedis_1 =========================&quot;); jedis_1.close(); //归还连接至连接池 &#125; catch (Throwable e) &#123; &#125; try &#123; logger.info(&quot;============== Sleep 10 ===============&quot;); Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 继续归还一个连接，空闲连接数：2 活跃连接数：1， // 等待后台执行剔除空闲连接，当前空闲连接数 &gt; 需要保持最小空闲连接数，进行剔除 try&#123; logger.info(&quot;=================== close jedis_2 =========================&quot;); jedis_2.close(); //归还连接至连接池 &#125; catch (Throwable e) &#123; &#125; try &#123; logger.info(&quot;============== Sleep 9 ===============&quot;); Thread.sleep(9000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; logger.info(&quot;-------------------------------pool close-----------------------------------&quot;); pool.close(); //关闭连接池 &#125; 硬闲置配置输出结果poolConfig.setMinEvictableIdleTimeMillis(3000); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 前三次获取成功=================== get jedis_1 =========================- makeJedis-- 132.122.232.225:9081- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 0 NumIdle: 0 NumWaiters: 0&#125;- =================== get jedis_2 =========================- makeJedis-- 132.122.232.225:9081- =================== get jedis_3 =========================- makeJedis-- 132.122.232.225:9081# 达到最大连接数，第四次获取失败，失败时间为4，与预期相同# NumWaiters = 1- =================== get jedis_4 =========================- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 3 NumIdle: 0 NumWaiters: 1&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 3 NumIdle: 0 NumWaiters: 1&#125;- Get jedis from ableNodes error at 132.122.232.225:9081 :Could not get a resource from the pool- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 3 NumIdle: 0 NumWaiters: 1&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 3 NumIdle: 0 NumWaiters: 1&#125;- Get jedis from ableNodes error at 132.122.232.225:9081 :Could not get a resource from the pool- Get resource cost time : 4003- jedis_4 : Get resource error# 归还一个连接- =================== close jedis_1 =========================- ============== Sleep 10 ===============- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒硬闲置时间，剔除，重新 makeJedis- makeJedis-- 132.122.232.225:9081- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒硬闲置时间，剔除，重新 makeJedis- makeJedis-- 132.122.232.225:9081- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;# 归还第二个连接- =================== close jedis_2 =========================- ============== Sleep 9 ===============# 未超过最大空闲连接数2，NumIdle+1 = 2- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 2 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 2 NumWaiters: 0&#125;# 到达3秒硬闲置时间，剔除，NumIdle - 1 = 1 = minIdle，不需要 makeJedis- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒硬闲置时间，剔除，重新 makeJedis- makeJedis-- 132.122.232.225:9081- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒硬闲置时间，剔除，重新 makeJedis- makeJedis-- 132.122.232.225:9081- nodePoolMap： &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- -------------------------------pool close----------------------------------- 软闲置配置输出结果poolConfig.setSoftMinEvictableIdleTimeMillis(3000); 123456789101112131415161718192021222324252627282930313233# 省略前面相同部分# 归还一个连接- =================== close jedis_1 =========================- ============== Sleep 10 ===============- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒软闲置时间，但 NumIdle = minIdle 不剔除，没有makeJedis日志输出- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒软闲置时间，但 NumIdle = minIdle 不剔除，没有makeJedis日志输出- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 2 NumIdle: 1 NumWaiters: 0&#125;# 归还第二个连接- =================== close jedis_2 =========================- ============== Sleep 9 ===============# 未超过最大空闲连接数2，NumIdle+1 = 2- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 2 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 2 NumWaiters: 0&#125;# NumIdle: 2到达3秒软闲置时间，NumIdle &gt; minIdle 剔除，- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒软闲置时间，但 NumIdle = minIdle 不剔除，没有makeJedis日志输出- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- nodePoolMap: &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;# NumIdle: 1到达3秒软闲置时间，但 NumIdle = minIdle 不剔除，没有makeJedis日志输出- nodePoolMap： &#123;132.122.232.225:9081=132.122.232.225:9081 NumActive: 1 NumIdle: 1 NumWaiters: 0&#125;- -------------------------------pool close----------------------------------- 从上述日志输出，验证了各参数的作用，其中 软闲置、硬闲置配置的区别，在于剔除闲置连接时，是否需要考虑当前 空闲连接与minIdle的大小关系。 此外，同时配置了软、硬闲置参数以后，输出结果与单独配置硬闲置参数相同，即设置MinEvictableIdleTimeMillis后，SoftMinEvictableIdleTimeMillis参数无效]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jedis</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB副本集详解2-数据同步]]></title>
    <url>%2F2018%2F03%2F16%2FMongoDB-repSet2%2F</url>
    <content type="text"><![CDATA[数据同步Mongodb副本集里的Secondary会从Primary上同步数据，以保持副本集所有节点的数据保持一致。MongoDB副本集数据同步主要包含2个步骤： init sync 可以理解为全量同步 oplog sync（replication）追同步源的oplog，可以理解为增量同步 新加入的Secondary先通过init sync同步Primary上的全量数据，再通过oplog sync不断重放Primary上的oplog同步增量数据。 init syncSecondary节点当出现如下状况时，需要先进行全量同步 oplog为空 local.replset.minvalid集合里_initialSyncFlag字段设置为true 内存标记initialSyncRequested设置为true 这3个场景分别对应 新节点加入，无任何oplog，此时需先进性initial sync initial sync开始时，会主动将_initialSyncFlag字段设置为true，正常结束后再设置为false；如果节点重启时，发现_initialSyncFlag为true，说明上次全量同步中途失败了，此时应该重新进行initial sync 当用户发送resync命令时，initialSyncRequested会设置为true，此时会重新开始一次initial sync intial sync流程 全量同步开始，设置minvalid集合的_initialSyncFlag 获取同步源上最新oplog时间戳为t1 全量同步集合数据，local库除外（耗时） 获取同步源上最新oplog时间戳为t2 重放[t1, t2]范围内的所有oplog 获取同步源上最新oplog时间戳为t3 重放[t2, t3]范围内所有的oplog 建立集合所有索引 （耗时） 获取同步源上最新oplog时间戳为t4 重放[t3, t4]范围内所有的oplog 全量同步结束，清除minvalid集合的_initialSyncFlag oplog sync（replication）initial sync结束后，Secondary会建立到Primary上local.oplog.rs的tailable cursor，不断从Primary上获取新写入的oplog，并应用到自身。 oplogPrimary与Secondary之间通过oplog来同步数据，Primary上的写操作完成后，会向特殊的 local.oplog.rs 特殊集合写入一条oplog，Secondary不断的从Primary取新的oplog并应用。 因oplog的数据会不断增加，local.oplog.rs被设置成为一个 capped集合 ，当容量达到配置上限时，会将最旧的数据删除掉。另外考虑到oplog在Secondary上可能重复应用，oplog必须具有幂等性，即重复应用也会得到相同的结果。 如下oplog的格式，包含ts、h、op、ns、o等字段1234567891011121314151617&#123; &quot;ts&quot; : Timestamp(1521169114, 4), &quot;t&quot; : NumberLong(11), &quot;h&quot; : NumberLong(-2875196737885853602), &quot;v&quot; : NumberInt(2), &quot;op&quot; : &quot;i&quot;, &quot;ns&quot; : &quot;testdb.table3&quot;, &quot;ui&quot; : BinData(4, &quot;A0pnjSbATe+O+H51myfqwQ==&quot;), &quot;wall&quot; : ISODate(&quot;2018-03-16T02:58:34.156+0000&quot;), &quot;o&quot; : &#123; &quot;_id&quot; : ObjectId(&quot;5aab32dc51382146343c0b03&quot;), &quot;id&quot; : NumberInt(3), &quot;type&quot; : &quot;database3&quot;, &quot;test&quot; : &quot;testval3&quot;, &quot;name&quot; : &quot;name3&quot; &#125;&#125; ts： 操作时间，当前timestamp + 计数器，计数器每秒都被重置 h：操作的全局唯一标识 v：oplog版本信息 op：操作类型 i：插入操作 u：更新操作 d：删除操作 c：执行命令（如createDatabase，dropDatabase） n：空操作，特殊用途 ns：操作针对的集合 o：操作内容，如果是更新操作 o2：操作查询条件，仅update操作包含该字段 oplog sync过程Tailable cursor每次会获取到一批oplog，Secondary采用多线程重放oplog以提高效率，通过将oplog按照所属的namespace进行分组，划分到多个线程里，保证同一个namespace的所有操作都由一个线程来replay，以保证统一namespace的操作时序跟primary上保持一致（如果引擎支持文档锁，只需保证同一个文档的操作时序与primary一致即可）。 producer thread，这个线程不断的从同步源上拉取oplog，并加入到一个BlockQueue的队列里保存着。 replBatcher thread，这个线程负责逐个从producer thread的队列里取出oplog，并放到自己维护的队列里。 sync线程将replBatcher thread的队列分发到默认16个replWriter线程，由replWriter thread来最终重放每条oplog。 拉取oplog是单线程进行，所以设计上producer thread只干一件事。 oplog重放时，要保持顺序性，而且遇到createCollection、dropCollection等DDL命令时，这些命令与其他的增删改查命令是不能并行执行的，而这些控制就是由replBatcher来完成的。 同一个namespace的所有操作都由一个线程来replay，以保证统一namespace的操作时序跟primary上保持一致 同步场景分析 副本集初始化 初始化选出Primary后，此时Secondary上无有效数据，oplog是空的，会先进行initial sync，然后不断的应用新的oplog 新成员加入 因新成员上无有效数据，oplog是空的，会先进行initial sync，然后不断的应用新的oplog 有数据的节点加入 有数据的节点加入有如下情况： 该节点与副本集其他节点断开连接，一段时间后恢复 该节点从副本集移除（处于REMOVED）状态，通过replSetReconfig命令将其重新加入 此时，如果该节点最新的oplog时间戳，比所有节点最旧的oplog时间戳还要小，该节点将找不到同步源，会一直处于RECOVERING而不能服务；反之，如果能找到同步源，则直接进入replication阶段，不断的应用新的oplog。 因oplog太旧而处于RECOVERING的节点目前无法自动恢复，需人工介入处理（故设置合理的oplog大小非常重要），最简单的方式是发送resync命令，让该节点重新进行initial sync。 oplog大小修改默认下，oplog大小会占用64位的实例5%的可用磁盘空间，在一些场景下oplog太小会导致同步失败等问题。动态修改oplog大小参考如下方法：1db.runCommand(&#123;collMod: &quot;oplog.rs&quot;, maxSize: 1024000000&#125;) 参考链接http://www.mongoing.com/archives/2369http://www.mongoing.com/archives/3076]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB副本集详解-角色与选举]]></title>
    <url>%2F2018%2F03%2F15%2FMongoDB-repSet%2F</url>
    <content type="text"><![CDATA[副本集简介Mongodb副本集由一组Mongod实例（进程）组成，包含一个Primary节点和多个Secondary节点，Mongodb Driver（客户端）的所有数据都写入Primary，Secondary从Primary同步写入的数据，以保持复制集内所有成员存储相同的数据集，提供数据的高可用。副本集带来的架构优点主要有： 集群高可用 读写可分离 副本集节点类型 Primary: 副本集的主节点，可读写，唯一可以进行写操作的节点，由集群自行选举出来。 Secondary： 正常情况下，Seconary会参与Primary选举（自身也可能会被选为Primary），并从Primary同步最新写入的数据，以保证与Primary存储相同的数据。Secondary可以提供读服务，增加Secondary节点可以提供副本集的读服务能力，提升副本集的可用性。 Arbiter： Arbiter节点只参与投票，不能被选为Primary，并且不从Primary同步数据。非常轻量级的服务，当复制集成员为偶数时，最好加入一个Arbiter节点，以提升复制集可用性。 Priority0： Priority0节点的选举优先级为0，不会被选举为Primary，且不能发起选举。 Vote0： 副本集成员最多50个，参与Primary选举投票的成员最多7个，其他成员（Vote0）的vote属性必须设置为0，即不参与投票。 Hidden： Hidden节点不能被选为主（Priority为0），并且对Driver不可见。因Hidden节点不会接受Driver的请求，可使用Hidden节点做一些数据备份、离线计算的任务，不会影响复制集的服务。 Delayed： Delayed节点必须是Hidden节点，并且其数据落后与Primary一段时间（可配置，比如1个小时）。因Delayed节点的数据比Primary落后一段时间，当错误或者无效的数据写入Primary时，可通过Delayed节点来做数据恢复。 Primary选举选举过程需要消耗一些时间，在此期间，集群将不能接收write操作（即使旧的primary仍然存活，但因为“网络分区”问题导致它不能与其他secondaries通讯），所有的members（包括旧的primary）都处于只读状态，选举对业务影响很大，所以需要尽量避免选举的发生。 需要进行Primary选举的场景： 副本集初始化时； 副本集被reconfig； Secondary节点检测到Primary宕机时； 当有Primary节点主动stepDown（主动降级为Secondary）时； Primary的选举受节点间心跳、优先级、最新的oplog时间等多种因素影响。 心跳： 复制集中的所有members之间都互相建立心跳连接，且每隔两秒发送一次心跳，如果未在10秒内收到回复，则此member将会被标记为“不可用”，Secondary（前提是可被选为Primary）会发起新的Primary选举，而其他能正常收到心跳反馈的Secondary能否决新的Primary选举。 优先级： 优先级即Priority值，每个member都有权重值，默认为都为1，members倾向于选举权重最高者。上述提到，priority为0的member不能被选举为primary且不能发起选举；只要当前primary的权重最高或者持有最新oplog数据的secondaries没有比它更高的权重时，集群不会触发选举。当Primary发现有优先级更高Secondary，并且该Secondary的数据落后在10s内，则Primary会主动降级，让优先级更高的Secondary有成为Primary的机会。 Optime： 当前member已经从primary的oplog中应用的最后一个operation的时间戳（此时间戳由primary生成，在oplog中每个操作记录都有）；一个member能成为primary的首要条件就是在所有有效的members中它持有最新的optime。 多数派连接： 一个member要成为primary，它必须与“多数派”的其他members建立连接，如果未能与足够多的member建立连接，事实上它本身也无法被选举为primary；多数派参考的是“总票数”，而不是member的个数，因为我们可以给每个member设定不同的“票数”。假设复制集内投票成员数量为N，则大多数为 N/2 + 1。 综上所述，在发起选举以后，能成为Primary的节点需要的条件有： 能够与“多数派”建立连接 在所有有效的members中它持有最新的optime 前两个条件相同的，Priority优先级高的成为Primary optime与Priority都相等时，谁发起选举，谁当选Primary 总结3.0版本以后副本集成员最多50个，参与Primary选举投票的成员最多7个，其他成员（Vote0）的vote属性必须设置为0，即不参与投票。 副本集中各类角色的特点总结如下： 节点类型 可读 可写 投票 oplog操作 当选Primary 否决 备注 Primary O O O 生成 — O 无 Secondary O X O 同步 O O 常规的Secondary Priority=0 O X O 同步 X O 无 Hidden X X O 同步 X O Priority=0，不可见 Delayed X X O 同步 X O 为Hidden，延迟同步 Arbiter X X O X X O Priority=0，无数据 vote=0 O X X 同步 O O 不能投票 备注： 上述Secondary为默认Secondary，即Priority！=0、vote！=0等； Hidden为特殊的Secondary，Delayed为特殊的Hidden 参考资料 https://docs.mongodb.com/manual/replication/ http://www.mongoing.com/archives/2155 http://blog.csdn.net/forevervip/article/details/43953189#7%E4%B8%AA%E6%8A%95%E7%A5%A8%E5%B8%AD%E4%BD%8D]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB：count结果不准确的原因与解决方法]]></title>
    <url>%2F2018%2F03%2F08%2FMongodb-Count%2F</url>
    <content type="text"><![CDATA[教训：MongoDB在分片后的集合上进行db.collection.count()操作时，出现结果不准确的现象，需要采用聚合的方法获取集合的count结果在使用MongoDB-Java客户端做简单的插入操作（10W条）以后，使用Studio 3T查看插入结果时，发现显示的count结果与插入的数据不一致，偶然会多出几条或十几条，插入操作很简单，其中table3被分片 12345678910111213141516171819//创建客户端连接MongoClient mongoClient = new MongoClient( &quot;xxx.xxx.xxx.xxx&quot; , 20000 );MongoDatabase database = mongoClient.getDatabase(&quot;testdb&quot;);MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;table3&quot;);//插入文档long start = System.currentTimeMillis();Random r = new Random();for (int i=0;i&lt;100000;i++)&#123; Document doc = new Document(&quot;id&quot;, i) .append(&quot;type&quot;, &quot;database&quot;+i) .append(&quot;test&quot;,&quot;testval&quot;+i) .append(&quot;name&quot;, &quot;name&quot;+i); System.out.println(&quot;insertOne = &quot; + i); collection.insertOne(doc);&#125;System.out.println(&quot;use time = &quot; + (System.currentTimeMillis()- start));System.out.println(&quot;count = &quot; + collection.count()); 发现问题后，通过在shell里面查询count，命令如下1db.table3.count() 使用该命令依然会出现统计信息不准确的现象，通过谷歌发现，官方文档——(https://docs.mongodb.com/manual/reference/method/db.collection.count/)中有解释这种现象： On a sharded cluster, db.collection.count() can result in an inaccurate count if orphaned documentsexist or if a chunk migration is in progress. To avoid these situations, on a sharded cluster, use the $group stage of the db.collection.aggregate() method to $sum the documents. For example, the following operation counts the documents in a collection: 12345db.collection.aggregate( [ &#123; $group: &#123; _id: null, count: &#123; $sum: 1 &#125; &#125; &#125; ]) 官方文档解释了这种现象的原因以及解决方法：不准确的原因： 操作的是分片的集合（前提）； shard分片正在做块迁移，导致有重复数据出现 存在孤立文档（因为不正常关机、块迁移失败等原因导致） 解决方法使用聚合aggregate的方式查询count数量，shell命令如下：12345db.collection.aggregate( [ &#123; $group: &#123; _id: null, count: &#123; $sum: 1 &#125; &#125; &#125; ]) java代码所以在Java中也可以采用聚合的方式获取count结果，使用聚合aggregate的方式可以准确的获取sharding后的集合的count结果。123456789101112DBObject groupFields = new BasicDBObject(&quot;_id&quot;, null);groupFields.put(&quot;count&quot;, new BasicDBObject(&quot;$sum&quot;, 1));BasicDBObject group = new BasicDBObject(&quot;$group&quot;, groupFields);List&lt;BasicDBObject&gt; aggreList = new ArrayList&lt;BasicDBObject&gt;();aggreList.add(group);AggregateIterable&lt;Document&gt; output = collection.aggregate(aggreList);for (Document dbObject : output)&#123; System.out.println(&quot;Aggregates count = &quot;+ dbObject);&#125; 完整代码12345678910111213141516171819202122232425262728293031323334//创建客户端连接MongoClient mongoClient = new MongoClient( &quot;xxx.xxx.xxx.xxx&quot; , 20000 );MongoDatabase database = mongoClient.getDatabase(&quot;testdb&quot;);MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;table3&quot;);//插入文档long start = System.currentTimeMillis();Random r = new Random();for (int i=0;i&lt;100000;i++)&#123; Document doc = new Document(&quot;id&quot;, i) .append(&quot;type&quot;, &quot;database&quot;+i) .append(&quot;test&quot;,&quot;testval&quot;+i) .append(&quot;name&quot;, &quot;name&quot;+i); System.out.println(&quot;insertOne = &quot; + i); collection.insertOne(doc);&#125;System.out.println(&quot;use time = &quot; + (System.currentTimeMillis()- start));System.out.println(&quot;count = &quot; + collection.count());DBObject groupFields = new BasicDBObject(&quot;_id&quot;, null);groupFields.put(&quot;count&quot;, new BasicDBObject(&quot;$sum&quot;, 1));BasicDBObject group = new BasicDBObject(&quot;$group&quot;, groupFields);List&lt;BasicDBObject&gt; aggreList = new ArrayList&lt;BasicDBObject&gt;();aggreList.add(group);AggregateIterable&lt;Document&gt; output = collection.aggregate(aggreList);for (Document dbObject : output)&#123; System.out.println(&quot;Aggregates count = &quot;+ dbObject);&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB（0）- 源码编译]]></title>
    <url>%2F2018%2F03%2F06%2FMongodbSourceInstall%2F</url>
    <content type="text"><![CDATA[虽然MongoDB官方提供了可执行的MongoDB供下载，但本着开源的工程都自己编译一次，于是从Git上的源码来编译MongoDB，官方的文档都是英文，中文文档找了几个博客，过程写的都不够全（可能是遇到的问题不一样）。在此记录一下本人从源码编译MongoDB的全过程，希望能对有心人有帮助。 环境要求官方文档docs/building.md中提出的要求如下 A modern and complete C++11 compiler. One of the following is required: VS2015 Update 2 or newer GCC 5.3.0 Clang 3.4 (or Apple XCode 5.1.1 Clang) or newer Python 2.7 SCons 2.3.5 or newer (for MSVC 2015 support) 可知，在centos中需要保证的环境有： gcc 5.3.0 python 2.7 scons 2.3.5 编译升级gcc我这台服务器为CentOS-7，上面的gcc版本比较低，版本如下：12345[root@localhost gcc-5.3.0]# g++ --versiong++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)Copyright © 2015 Free Software Foundation, Inc.本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；包括没有适销性和某一专用目的下的适用性担保。 我使用了一个ftp镜像站下载GCC 5.3.0的源码压缩包。 然后进行解压和安装依赖。 解压gcc-5.3.0.tar.gz12# 在/opt/gcc-5.3.0目录中安装gcctar -xvf gcc-5.3.0.tar.gz 下载安装依赖,下载安装gcc需要的三个依赖12cd gcc-5.3.0/./contrib/download_prerequisites (在解压根目录中执行) 注意 上一步下载依赖，需要使用wget命令、解压bz2文件等，确保安装了命令具，安装方法如下：12yum -y install bzip2yum -y install wget 依赖下载完成后，编译，更新gcc版本123456789cd ../ &amp;&amp; mkdir gcc-build-5.3.0 &amp;&amp; cd gcc-build-5.3.0# configure../gcc-5.3.0/configure --enable-checking=release --enable-languages=c,c++ --disable-multilib# 编译make # 需要等待很久，视机器性能而定make install# 切换gcc到新版本update-alternatives --install /usr/bin/gcc gcc /opt/gcc-5.3.0/gcc-5.3.0 我重新ssh登录后看到更新生效12345[root@localhost ~]# g++ --versiong++ (GCC) 5.3.0Copyright © 2015 Free Software Foundation, Inc.本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；包括没有适销性和某一专用目的下的适用性担保。 安装python、scons查看python版本，满足要求，不做修改。12[root@localhost gcc-5.3.0]# python -VPython 2.7.5 安装scons，下载地址,下载scons-3.0.1.tar.gz版本。12345# 依赖解决yum install pcre-devel python-devel# 解压安装tar -zxvf scons-3.0.1.tar.gz &amp;&amp; cd scons-3.0.1python setup.py install 验证scons安装是否成功，输入scons -h会显示提示信息。123456789101112[root@localhost gcc-5.3.0]# scons -husage: scons [OPTION] [TARGET] ...SCons Options: -b, -d, -e, -m, -S, -t, -w, --environment-overrides, --no-keep-going, --no-print-directory, --print-directory, --stop, --touch Ignored for compatibility. -c, --clean, --remove Remove specified targets and dependencies. -C DIR, --directory=DIR Change to DIR before doing anything. --cache-debug=FILE Print CacheDir debug info to FILE. --cache-disable, --no-cache # 以下省略 编译安装MongoDB从git上下载MongoDB源代码1234567# 下载源码git clone git://github.com/mongodb/mongo.gitcd mongo# 列出所有版本git tag -l # 检出 需要安装的版本git checkout r3.6.3 官方文档中介绍，linux下编译需要安装openssl-develOn Linux, you will need to install a compiler gcc or clang, as well as glibc headers which are usually included in a package named glibc-devel. On Debian and Ubuntu systems, you must install the libssl-dev package to compile with SSL support. On Red Hat and CentOS systems, you must install the openssl-devel package to compile with SSL support. openssl-devel安装命令如下：1yum install openssl openssl-devel 安装python 依赖包，通过在代码根目录中执行如下命令12# 在mongoDB解压根目录中执行pip install -r buildscripts/requirements.txt 编译MongoDB源代码并安装 注意：本人开始编译所有组件，等待很久以后磁盘空间不够用，退出时编译已占用了12G空间以上，若需要编译所有，请留意磁盘大小。12345678910# 编译所有组件，需要很大磁盘空间，注意虚拟机磁盘大小scons all# 编译核心组件 mongod, mongos, shellscons core# 安装（ 安装到/opt/mongo ）scons --prefix=/opt/mongo install# 如果需要lib库和include头文件 需要加上 –full 参数如下：# scons –prefix=/opt/mongo –full install]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>源码</tag>
        <tag>编译安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB集群部署：副本集、分片]]></title>
    <url>%2F2018%2F02%2F25%2FMongoDB-install%2F</url>
    <content type="text"><![CDATA[本文主要介绍MongoDB集群部署的过程，对于其中涉及的组件概念，不做解释。 服务器规划三台服务器：xxx.xxx.xxx.234、xxx.xxx.xxx.235、xxx.xxx.xxx.236 服务器 234 235 235 组件1 mongos mongos mongos 组件2 config server config server config server 组件3 shard server1主 shard server1副 shard server1仲裁 组件4 shard server2仲裁 shard server2主 shard server2副 组件5 shard server3副 shard server3仲裁 shard server3主 三台机器的端口配置都采用如下方式： mongos：20000 config：21000 shard1：27001 shard2：27002 shard3：27003 安装mongodb解压，重命名1234#解压tar -xzvf mongodb-linux-x86_64-3.6.3.tgz -C /root/mongo#改名mv mongodb-linux-x86_64-3.6.3 mongodb 配置环境变量 vim /etc/profile123456# /etc/profile添加内容export MONGODB_HOME=/root/mongo/mongodbexport PATH=$MONGODB_HOME/bin:$PATH# 使立即生效source /etc/profile 分别在每台机器建立conf、mongos、config、shard1、shard2、shard3六个目录，因为mongos不存储数据，只需要建立日志文件目录即可。12345678910mkdir -p /root/mongo/data/confmkdir -p /root/mongo/ data /mongos/logmkdir -p /root/mongo/ data /config/datamkdir -p /root/mongo/ data /config/logmkdir -p /root/mongo/ data /shard1/datamkdir -p /root/mongo/ data /shard1/logmkdir -p /root/mongo/ data /shard2/datamkdir -p /root/mongo/ data /shard2/logmkdir -p /root/mongo/ data /shard3/datamkdir -p /root/mongo/ data /shard3/log config servermongodb3.4以后要求配置服务器也创建副本集，不然集群搭建不成功。添加配置文件1234567891011121314151617pidfilepath = /root/mongo/data/config/log/configsrv.piddbpath = /root/mongo/data/config/datalogpath = /root/mongo/data/config/log/congigsrv.loglogappend = true bind_ip = 0.0.0.0port = 21000fork = true #declare this is a config db of a cluster;configsvr = true#副本集名称replSet=configs #设置最大连接数maxConns=20000 分别启动三台服务器的config server1mongod -f /usr/local/mongodb/conf/config.conf 登录任意一台配置服务器，初始化配置副本集1234567891011121314#连接mongo --port 21000#config变量config = &#123;... _id : &quot;configs&quot;,... members : [... &#123;_id : 0, host : &quot;xxx.xxx.xxx.234:21000&quot; &#125;,... &#123;_id : 1, host : &quot;xxx.xxx.xxx.235:21000&quot; &#125;,... &#123;_id : 2, host : &quot;xxx.xxx.xxx.236:21000&quot; &#125;... ]... &#125;#初始化副本集rs.initiate(config) 其中，”_id” : “configs”应与配置文件中配置的 replicaction.replSetName 一致，”members” 中的 “host” 为三个节点的 ip 和 port shard分片副本集第1个分片副本集在/root/mongo/data/conf中新增配置文件 vi shard1.conf12345678910111213141516171819pidfilepath = /root/mongo/data/shard1/log/shard1.piddbpath = /root/mongo/data/shard1/datalogpath = /root/mongo/data/shard1/log/shard1.loglogappend = truebind_ip = 0.0.0.0port = 27001fork = truedirectoryperdb = true#副本集名称replSet=shard1#declare this is a shard db of a cluster;shardsvr = true#设置最大连接数maxConns=20000 启动三台服务器的shard1 server1mongod -f /usr/local/mongodb/conf/shard1.conf 登陆任意一台服务器（非仲裁节点），初始化副本集1234567891011121314mongo --port 27001#使用admin数据库use admin#定义副本集配置，第三个节点的 &quot;arbiterOnly&quot;:true 代表其为仲裁节点。config = &#123;... _id : &quot;shard1&quot;,... members : [... &#123;_id : 0, host : &quot;xxx.xxx.xxx.234:27001&quot; &#125;,... &#123;_id : 1, host : &quot;xxx.xxx.xxx.235:27001&quot; &#125;,... &#123;_id : 2, host : &quot;xxx.xxx.xxx.236:27001” , arbiterOnly: true &#125;... ]... &#125;#初始化副本集配置rs.initiate(config); 第2个分片副本集在/root/mongo/data/conf中新增配置文件 vi shard2.conf12345678910111213141516171819pidfilepath = /root/mongo/data/shard2/log/shard2.piddbpath = /root/mongo/data/shard2/datalogpath = /root/mongo/data/shard2/log/shard2.loglogappend = truebind_ip = 0.0.0.0port = 27002fork = truedirectoryperdb = true #副本集名称replSet=shard2 #declare this is a shard db of a cluster;shardsvr = true #设置最大连接数maxConns=20000 分别启动三台服务器的shard2 server1mongod -f /usr/local/mongodb/conf/shard2.conf 登陆任意一台服务器（非仲裁节点），初始化副本集123456789101112131415mongo --port 27002#使用admin数据库use admin#定义副本集配置config = &#123;... _id : &quot;shard2&quot;,... members : [... &#123;_id : 0, host : &quot;xxx.xxx.xxx.234:27002&quot; , arbiterOnly: true &#125;,... &#123;_id : 1, host : &quot;xxx.xxx.xxx.235:27002&quot; &#125;,... &#123;_id : 2, host : &quot;xxx.xxx.xxx.236:27002&quot; &#125;... ]... &#125;#初始化副本集配置rs.initiate(config); 第3个分片副本集在/root/mongo/data/conf中新增配置文件 vi shard3.conf12345678910111213141516171819pidfilepath = /root/mongo/data/shard3/log/shard3.piddbpath = /root/mongo/data/shard3/datalogpath = /root/mongo/data/shard3/log/shard3.loglogappend = truebind_ip = 0.0.0.0port = 27003fork = truedirectoryperdb = true #副本集名称replSet=shard3 #declare this is a shard db of a cluster;shardsvr = true #设置最大连接数maxConns=20000 启动三台服务器的shard3 server1mongod -f /usr/local/mongodb/conf/shard3.conf 登陆任意一台服务器（非仲裁节点），初始化副本集123456789101112131415mongo --port 27003#使用admin数据库use admin#定义副本集配置config = &#123;... _id : &quot;shard3&quot;,... members : [... &#123;_id : 0, host : &quot;xxx.xxx.xxx.234:27003&quot; &#125;,... &#123;_id : 1, host : &quot;xxx.xxx.xxx.235:27003&quot; , arbiterOnly: true&#125;,... &#123;_id : 2, host : &quot;xxx.xxx.xxx.236:27003&quot; &#125;... ]... &#125;#初始化副本集配置rs.initiate(config); mongos路由服务器先启动配置服务器和分片服务器,后启动路由实例:（三台机器）在/root/mongo/data/conf中新增配置文件 vi mongos.conf 12345678910111213pidfilepath = /root/mongo/data/mongos/log/mongos.pidlogpath = /root/mongo/data/mongos/log/mongos.loglogappend = truebind_ip = 0.0.0.0port = 20000fork = true#监听的配置服务器,只能有1个或者3个 configs为配置服务器的副本集名字configdb = configs/xxx.xxx.xxx.234:21000,xxx.xxx.xxx.235:21000,xxx.xxx.xxx.236:21000#设置最大连接数maxConns=20000 启动三台服务器的mongos server1mongos -f /usr/local/mongodb/conf/mongos.conf 启用分片与测试目前搭建了mongodb配置服务器、路由服务器，各个分片服务器，不过应用程序连接到mongos路由服务器并不能使用分片机制，还需要在程序里设置分片配置，让分片生效。登陆任意一台mongos123456789mongo --port 20000#使用admin数据库use admin#串联路由服务器与分配副本集sh.addShard(&quot;shard1/xxx.xxx.xxx.234:27001,xxx.xxx.xxx.235:27001, xxx.xxx.xxx.236:27001&quot;)sh.addShard(&quot;shard2/xxx.xxx.xxx.234:27002,xxx.xxx.xxx.235:27002, xxx.xxx.xxx.236:27002&quot;)sh.addShard(&quot;shard3/xxx.xxx.xxx.234:27003,xxx.xxx.xxx.235:27003, xxx.xxx.xxx.236:27003&quot;)#查看集群状态sh.status() 目前配置服务、路由服务、分片服务、副本集服务都已经串联起来了，但我们的目的是希望插入数据，数据能够自动分片。连接在mongos上，准备让指定的数据库、指定的集合分片生效。默认的chunkSize为64M，这里的数据量没有达到64M，可以修改一下chunkSize的大小为1M，方便测试分片效果：12345678mongo --port 20000mongos&gt; use configswitched to db configmongos&gt; db.settings.save( &#123; _id:&quot;chunksize&quot;, value: 1 &#125; )WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : &quot;chunksize&quot; &#125;)mongos&gt; db.settings.find();&#123; &quot;_id&quot; : &quot;balancer&quot;, &quot;stopped&quot; : false, &quot;mode&quot; : &quot;full&quot; &#125;&#123; &quot;_id&quot; : &quot;chunksize&quot;, &quot;value&quot; : 1 &#125; 指定testdb分片生效1234567#指定testdb分片生效mongos&gt; db.runCommand( &#123; enablesharding :&quot;testdb&quot;&#125;);#指定数据库里需要分片的集合和片键mongos&gt; db.runCommand( &#123; shardcollection : &quot;testdb.table1&quot;,key : &#123;id: 1&#125; &#125; )mongos&gt; use testdbswitched to db testdb 使用java代码插入10W调数据测试分片效果1234567891011121314//创建客户端连接MongoClient mongoClient = new MongoClient( &quot;xxx.xxx.xxx.234&quot; , 20000 );MongoDatabase database = mongoClient.getDatabase(&quot;testdb&quot;);MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;table1&quot;);//插入文档for (int i=1;i&lt;100000;i++)&#123; Document doc = new Document(&quot;id&quot;, i) .append(&quot;type&quot;, &quot;database&quot;) .append(&quot;test&quot;,&quot;testval&quot;); System.out.println(i); collection.insertOne(doc);&#125; 使用db.table1.stats();命令查看集合分片状态，部分无关信息省掉，可以看到数据分到3个分片。 参考文献 http://www.ityouknow.com/mongodb/2017/08/05/mongodb-cluster-setup.html]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>安装部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flux模式开发提交JStorm任务]]></title>
    <url>%2F2017%2F12%2F02%2Fjstormflux%2F</url>
    <content type="text"><![CDATA[传统法式采用提交jar包的方式运行topology，一旦我们需要改变拓扑里头的相应配置，我们就必须重新编译和打包，而Flux可以帮助我们创建和部署jstorm拓扑的编程框架及组件。它可以将你代码中有关topology结构以及提交部分用一句话加上配置文件完成。 传统方式在jar内完成topology的构建以及数据流配置，代码可能如下：1234567891011121314151617TopologyBuilder builder = new TopologyBuilder();builder.setSpout(&quot;send&quot;,new genRandomSentenceSpout());builder.setBolt(&quot;split&quot;,new splitSentenceBolt()).shuffleGrouping(&quot;send&quot;); builder.setBolt(&quot;count&quot;,new wordCountBolt()).fieldsGrouping(&quot;split&quot;,new Fields(&quot;word&quot;));Config conf=new Config();conf.setNumWorkers(1);conf.setNumAckers(1);boolean runLocal = shouldRunLocal();if(runLocal)&#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology(name, conf, builder.createTopology()); //本地提交&#125; else &#123; StormSubmitter.submitTopology(name, conf, builder.createTopology()); //集群提交 &#125;&#125; 使用Flux，上面代码可用如下Flux命令代替：12jstorm jar mytopology.jar com.alibaba.jstorm.flux.Flux --local config.yaml //本地提交jstorm jar mytopology.jar com.alibaba.jstorm.flux.Flux --remote config.yaml //远程提交 Flux方式开发maven依赖与打包配置由于需要maven依赖flux-core，而flux-core在网上没有链接可以下载，所以需要手动生产安装。通过集群版本下载对应JStorm源码，maven中编译安装JStorm-Flux，会在你本地maven仓库中安装jstorm-core.jar。 然后在开发topology项目中添加maven依赖：1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.jstorm&lt;/groupId&gt; &lt;artifactId&gt;flux-core&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 如下代码以maven-shade打包为例，在pom.xml中添加打包方式，其中mainClass设置为com.alibaba.jstorm.flux.Flux123456789101112131415161718192021222324252627&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot; /&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;com.alibaba.jstorm.flux.Flux&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 配置文件开发完spout、bolt后不需要在main函数中显示配置topology的结构，采用配置文件的方式来构建topology结构。例如如下的代码跟配置文件在效果上是一样的。1234567891011//代码方式构建topologyTopologyBuilder builder = new TopologyBuilder();builder.setSpout(&quot;send&quot;,new genRandomSentenceSpout());builder.setBolt(&quot;split&quot;,new splitSentenceBolt()).shuffleGrouping(&quot;send&quot;); builder.setBolt(&quot;count&quot;,new wordCountBolt()).fieldsGrouping(&quot;split&quot;,new Fields(&quot;word&quot;));Config conf=new Config();conf.setNumWorkers(1);conf.setNumAckers(1);StormSubmitter. submitTopology(topo_name , conf, builder.createTopology() ); 123456789101112131415161718192021222324252627282930313233343536# Flux配置文件方式---# 定义topology名name: &quot;flux&quot;# topology有关配置，worker、acker数量配置config: topology.workers: 1 topology.ackers: 1# spouts配置spouts: - id: &quot;word-spout&quot; className: &quot;spout.genRandomSentenceSpout&quot;parallelism: 1# Bolt配置bolts: - id: &quot;word-counter&quot; className: &quot;bolt.wordCountBolt&quot; parallelism: 1 - id: &quot;split-bolt&quot; className: &quot;bolt.splitSentenceBolt&quot; parallelism: 1# 数据流配置streams: - name: &quot;word-spout --&gt; split-bolt&quot; # name isn&apos;t used (placeholder for logging, UI, etc.) from: &quot;word-spout&quot; to: &quot;split-bolt&quot; grouping: type: SHUFFLE - name: &quot;split-bolt --&gt; word-counter&quot; from: &quot;split-bolt&quot; to: &quot;word-counter&quot; grouping: type: SHUFFLE args: [&quot;word&quot;] 发布提交一旦你用flux完成了topology打包，你就可以利用配置文件来跑各种拓扑啦。比如你的jar名称为myTopology-0.1.0-SNAPSHOT.jar， 你可以利用以下命令跑本地模式1jstorm jar myTopology-0.1.0-SNAPSHOT.jar com.alibaba.jstorm.flux.Flux --local my_config.yaml 当然你也可以跑分布式模式1jstorm jar myTopology-0.1.0-SNAPSHOT.jar com.alibaba.jstorm.flux.Flux --remote my_config.yaml]]></content>
      <categories>
        <category>JStorm</category>
      </categories>
      <tags>
        <tag>JStorm</tag>
        <tag>Storm</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JStorm运行依赖storm-core的任务：依赖冲突与解决]]></title>
    <url>%2F2017%2F11%2F29%2FJStorm%E8%BF%90%E8%A1%8C%E4%BE%9D%E8%B5%96storm-core%E7%9A%84%E4%BB%BB%E5%8A%A1%EF%BC%9A%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81%E4%B8%8E%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[JStorm虽然用Java语言重新实现了storm，但是对于storm的external部分并未实现。现在在JStorm基础上构建topology时，需要使用storm的external部分中的storm-sql-core以及storm-sql-runtime，这样在构建topology的项目中需要同时依赖storm-core、jstorm-core，出现了冲突。 本地调试问题Found multiple defaults.yaml resources根据前面描述的情况，maven的pom.xml文件将包含如下的依赖jstorm-core、storm-core、storm-sql-core、storm-sql-runtime。123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;com.alibaba.jstorm&lt;/groupId&gt; &lt;artifactId&gt;jstorm-core&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;!--本地调试时注释一下scope --&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-sql-core&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-sql-runtime&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;/dependency&gt; 此时按照JStorm本地调试的模式在ide中运行，12345LocalCluster cluster = new LocalCluster();cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());Utils.sleep(20000);cluster.killTopology(TOPOLOGY_NAME);cluster.shutdown(); 出现如下报错：123456789101112131415161718192021Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerErrorat backtype.storm.topology.BaseConfigurationDeclarer.(BaseConfigurationDeclarer.java:29)at backtype.storm.topology.TopologyBuilder$ConfigGetter.(TopologyBuilder.java:433)at backtype.storm.topology.TopologyBuilder$SpoutGetter.(TopologyBuilder.java:450)at backtype.storm.topology.TopologyBuilder.setSpout(TopologyBuilder.java:289)at com.ctg.itrdc.ruleengine.JsonTopology.main(JsonTopology.java:34)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:498)at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Caused by: java.lang.RuntimeException: Invalid configuration defaults.yaml:Found multiple defaults.yaml resources. You&apos;re probably bundling the Storm jars with your topology jar. [jar:file:/E:/Program%20Files%20(x86)/apache-maven-3.3.1-bin/repos/repository/com/alibaba/jstorm/jstorm-core/2.2.1/jstorm-core-2.2.1.jar!/defaults.yaml, jar:file:/E:/Program%20Files%20(x86)/apache-maven-3.3.1-bin/repos/repository/org/apache/storm/storm-core/1.1.1/storm-core-1.1.1.jar!/defaults.yaml]at com.alibaba.jstorm.utils.LoadConf.findAndReadYaml(LoadConf.java:77)at backtype.storm.utils.Utils.readDefaultConfig(Utils.java:355)at backtype.storm.utils.Utils.readStormConfig(Utils.java:453)at backtype.storm.utils.Utils.(Utils.java:112)... 10 moreCaused by: java.io.IOException: Found multiple defaults.yaml resources. You&apos;re probably bundling the Storm jars with your topology jar. [jar:file:/E:/Program%20Files%20(x86)/apache-maven-3.3.1-bin/repos/repository/com/alibaba/jstorm/jstorm-core/2.2.1/jstorm-core-2.2.1.jar!/defaults.yaml, jar:file:/E:/Program%20Files%20(x86)/apache-maven-3.3.1-bin/repos/repository/org/apache/storm/storm-core/1.1.1/storm-core-1.1.1.jar!/defaults.yaml]at com.alibaba.jstorm.utils.LoadConf.getConfigFileInputStream(LoadConf.java:101)at com.alibaba.jstorm.utils.LoadConf.findAndReadYaml(LoadConf.java:55)... 13 more 可以看出是因为同时依赖了jstorm-core、storm-core导致存在多个配置文件加载出错。 解决方法解决的方法比较暴力，pom.xml文件不需要做修改，保持jstorm-core、storm-core等被注释了，找到冲突的本地maven仓库中的/repository/org/apache/storm/storm-core/1.1.1/storm-core-1.1.1.jar，将jar包下的defaults.yaml删除。采用本地模式提交topology，注意有关提交topology的类，依赖自JStorm中的import backtype.storm.xxx、不要继承自import org.apache.storm.xxx。以上，主要是删除storm中有冲突的配置文件defaults.yaml。 集群模式问题问题1:Invalid signature file digest for Manifest main attributes修改pom.xml，将jstorm-core、storm-core设置为provided，修改本地提交代码为集群提交：StormSubmitter. submitTopology(topo_name , config, builder.createTopology() );打包后去集群上执行，提交失败，报错：Exception in thread “main” java.lang.SecurityException: Invalid signature file digest for Manifest main attributes12345678910111213141516171819202122232425262728293031323334353637$ ./jstorm jar ~/topology/ruleengine-0.0.1-SNAPSHOT.jar com.ctg.itrdc.ruleengine.JsonTopology ruleengine2/usr/java/jdk1.8.0_111/bin/javaException in thread &quot;main&quot; java.lang.ExceptionInInitializerError at backtype.storm.command.config_value.main(config_value.java:40)Caused by: java.lang.SecurityException: Invalid signature file digest for Manifest main attributes at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314) at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268) at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316) at java.util.jar.JarVerifier.update(JarVerifier.java:228) at java.util.jar.JarFile.initializeVerifier(JarFile.java:383) at java.util.jar.JarFile.getInputStream(JarFile.java:450) at sun.misc.URLClassPath$JarLoader$2.getInputStream(URLClassPath.java:940) at sun.misc.Resource.cachedInputStream(Resource.java:77) at sun.misc.Resource.getByteBuffer(Resource.java:160) at java.net.URLClassLoader.defineClass(URLClassLoader.java:454) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at backtype.storm.utils.Utils.&lt;clinit&gt;(Utils.java:103) ... 1 moreFailed to get config java.library.pathNoneruleengine2cannot concatenate &apos;str&apos; and &apos;NoneType&apos; objectsSyntax: [jstorm jar topology-jar-path class ...] Runs the main method of class with the specified arguments. The jstorm jars and configs in $JSTORM_CONF_DIR/storm.yaml are put on the classpath. The process is configured so that StormSubmitter (https://github.com/alibaba/jstorm/wiki/JStorm-Chinese-Documentation) will upload the jar at topology-jar-path when the topology is submitted. 解决方法1谷歌后发现，Exception in thread “main” java.lang.SecurityException: Invalid signature file digest for Manifest main attributes错误是maven打包时设置的问题，修改打包配置如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;1.7.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.AppendingTransformer&quot;&gt; &lt;resource&gt;META-INF/spring.handlers&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.AppendingTransformer&quot;&gt; &lt;resource&gt;META-INF/spring.schemas&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;com.ctg.itrdc.ruleengine.JsonTopology&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主要是添加如下部分。12345678910&lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt;&lt;/filters&gt; 问题 2:storm-core类加载失败按照在Git上的看到的回复，集群运行时，设置依赖的时候jstorm和storm的依赖应该都设置成provided。解决了问题1后，成功提交topology，能够成功运行，在web-UI中查看日志也没有出错，但是topology运行的结果不对。 猜测是storm-sql-core以及storm-sql-runtime依赖了storm-core，但是因为jstorm-core、storm-core设置为provided，但是JStorm集群中只有jstorm-core是provided，而storm-core仍然是没有提供。 坑的是类加载失败错误是直接system.out.print，而不是会打印log，所以在Web-UI中是看不到具体错误的。继续按照猜测走下去，在pom.xml中将storm-core的provided注释掉:1&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; 打包提交，成功加载了storm-core的类，且topology正常运行没有报错,问题解决。]]></content>
      <categories>
        <category>JStorm</category>
      </categories>
      <tags>
        <tag>JStorm</tag>
        <tag>Storm</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何本地调试-JStorm程序]]></title>
    <url>%2F2017%2F11%2F15%2F%E5%A6%82%E4%BD%95%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95-JStorm-%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[JStorm 提供了两种运行模式：本地模式和分布式模式。本地模式针对开发调试storm topologies非常有用。 如果你还在用日常的web ui提交拓扑这种远古的方式进行调试测试，那就赶快阅读本文吧。本文将介绍在本机不安装JStorm环境的情况下，开发、调试JStorm程序。 单机模式主要是在代码中加入：import backtype.storm.LocalCluster; LocalCluster cluster = new LocalCluster(); //建议加上这行，使得每个bolt/spout的并发度都为1 conf.put(Config.TOPOLOGY_MAX_TASK_PARALLELISM, 1); //提交拓扑 cluster.submitTopology("SequenceTest", conf, builder.createTopology()); //等待1分钟， 1分钟后会停止拓扑和集群， 视调试情况可增大该数值 Thread.sleep(60000); //结束拓扑 cluster.killTopology("SequenceTest"); cluster.shutdown(); 用LocalCluster来模拟集群环境，你可以在LocalCluster对象上调用submitTopology方法来提交拓扑，submitTopology(String topologyName, Map conf, StormTopology topology)接受一个拓扑名称，一个拓扑的配置，以及一个拓扑的对象。就像StormSubmitter一样。你还可以调用killTopology来结束一个拓扑。对应的还有active,deactive,rebalance等方法。由于JStorm是个不会停止的程序，所以我们最后需要显示地停掉集群。 修改pom.xml以jstorm 2.1.1版本为例。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.jstorm&lt;/groupId&gt; &lt;artifactId&gt;jstorm-core&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;!-- keep jstorm out of the jar-with-dependencies --&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt; &lt;/dependency&gt; 注意要注释掉jstorm依赖中的&lt;scope&gt;provided&lt;/scope&gt;，而提交的时候必须记得将这行改回来！ 否则会报多个defaults.yaml的错误。 注：如果依赖的是 0.9.x 版本的jstorm，会有三个依赖包，将这三个依赖的provided都注释掉。 Re-import 项目， 然后运行main class就可以了。为了更好的代码组织，建议将本地运行和集群运行写成两个方法，根据参数/配置来调用不同的运行方式。更多可以参照SequenceTopology的例子 注意点本地调试主要是用于测试应用逻辑的，因此有一些限制，如classloader是不起作用的。此外，还需要注意一下你的应用中log4j的依赖，如果应用的依赖中自带了log4j.properties，则有可能导致将jstorm默认的本地测试的log4j配置覆盖掉，从而导致调试时控制台没有任何输出。]]></content>
      <categories>
        <category>JStorm</category>
      </categories>
      <tags>
        <tag>JStorm</tag>
        <tag>大数据</tag>
        <tag>java</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JStorm：单词计数-开发示例]]></title>
    <url>%2F2017%2F09%2F12%2FWordCountTopology%2F</url>
    <content type="text"><![CDATA[JStorm：概念与编程模型JStorm：任务调度示例功能说明：统计单词出现的次数，spout将持续输入的一句句话作为输入流，bolt将一句话分割成单词，最后统计每个单词出现的次数。 示例介绍如下图所示，单词计数topology由一个spout和下游三个bolt组成。SentenceSpout：向后端发射一个单值tuple组成的数据流，键名“sentence”，tuple如下：{“sentence”：“my name is zhangsan”}SplitSentenceBolt：订阅SentenceSpout发射的数据流，将“sentence”中的语句分割为一个个单词，向后端发射“word”组成的tuple如下：{“word”：“my”}{“word”：“name”}{“word”：“is”}{“word”：“zhangsan”}WordCountBolt：订阅SplitSentenceBolt发射的数据流，保存每个特定单词出现的次数，每当bolt收到一个tuple，将对应单词的计数加一，并想后发射该单词当前的计数。{“word”:“my”,“count”:“5”}ReportBolt：订阅WordCountBolt的输出流，维护一份所有单词对应的计数表，结束时将所有值打印。 代码实现添加Pom.xml依赖123456&lt;dependency&gt; &lt;groupId&gt;com.alibaba.jstorm&lt;/groupId&gt; &lt;artifactId&gt;jstorm-core&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;&lt;/dependency&gt; SentenceSpout：继承BaseRichSpout类，在nextTuple方法中生成并向后发射数据流，declareOutputFields方法定义了向后发射数据流tuple的字段名为：sentence。SplitSentenceBolt：继承BaseRichBolt类，在execute方法中将接收到的tuple分割为单词，并向后传输tuple，declareOutputFields定义了tuple字段为word。WordCountBolt：继承BaseRichBolt，在execute方法中统计单词出现的次数，本地使用HashMap保存所有单词出现的次数。接收到tuple后更新该单词出现的次数并向后传输tuple，declareOutputFields定义了tuple为”word”, “count”。ReportBolt：继承BaseRichBolt类，在execute方法中汇总所有单词出现的次数。本地使用HashMap保存所有单词出现的次数。当任务结束时，Cleanup方法打印统计结果。WordCountTopology：创建topology，定义了Spout以及Bolt之间数据流传输的规则，以及并发数（前后并发为2、2、4、1）。进程（worker）、线程（Executor）与Task之间的关系如下图：核心代码参考如下，注意其中的shuffleGrouping设定向后传输数据流为随机，fieldsGrouping按照字段值向后传输数据流，能保证同一个单词由同一个WordCountBolt统计，而globalGrouping保证汇总的bolt是单例。 WordCountTopology.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//WordCountTopology代码import storm.blueprints.word.v1.*;import backtype.storm.Config;import backtype.storm.LocalCluster;import backtype.storm.topology.TopologyBuilder;import backtype.storm.tuple.Fields;import static storm.blueprints.utils.Utils.*;public class WordCountTopology &#123; private static final String SENTENCE_SPOUT_ID = &quot;sentence-spout&quot;; private static final String SPLIT_BOLT_ID = &quot;split-bolt&quot;; private static final String COUNT_BOLT_ID = &quot;count-bolt&quot;; private static final String REPORT_BOLT_ID = &quot;report-bolt&quot;; private static final String TOPOLOGY_NAME = &quot;word-count-topology&quot;; public static void main(String[] args) throws Exception &#123; SentenceSpout spout = new SentenceSpout(); SplitSentenceBolt splitBolt = new SplitSentenceBolt(); WordCountBolt countBolt = new WordCountBolt(); ReportBolt reportBolt = new ReportBolt(); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(SENTENCE_SPOUT_ID, spout, 2); // SentenceSpout --&gt; SplitSentenceBolt builder.setBolt(SPLIT_BOLT_ID, splitBolt, 2) .setNumTasks(4) .shuffleGrouping(SENTENCE_SPOUT_ID); // SplitSentenceBolt --&gt; WordCountBolt builder.setBolt(COUNT_BOLT_ID, countBolt, 4) .fieldsGrouping(SPLIT_BOLT_ID, new Fields(&quot;word&quot;)); // WordCountBolt --&gt; ReportBolt builder.setBolt(REPORT_BOLT_ID, reportBolt) .globalGrouping(COUNT_BOLT_ID); Config config = new Config(); config.setNumWorkers(2); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology()); waitForSeconds(10); cluster.killTopology(TOPOLOGY_NAME); cluster.shutdown(); &#125;&#125; SentenceSpout.java12345678910111213141516171819202122232425262728293031323334353637383940import backtype.storm.spout.SpoutOutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichSpout;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import storm.blueprints.utils.Utils;import java.util.Map;public class SentenceSpout extends BaseRichSpout &#123; private SpoutOutputCollector collector; private String[] sentences = &#123; &quot;my dog has fleas&quot;, &quot;i like cold beverages&quot;, &quot;the dog ate my homework&quot;, &quot;don&apos;t have a cow man&quot;, &quot;i don&apos;t think i like fleas&quot; &#125;; private int index = 0; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;sentence&quot;)); &#125; public void open(Map config, TopologyContext context, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; public void nextTuple() &#123; this.collector.emit(new Values(sentences[index])); index++; if (index &gt;= sentences.length) &#123; index = 0; &#125; Utils.waitForMillis(1000); &#125;&#125; SplitSentenceBolt.java12345678910111213141516171819202122232425262728import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Tuple;import backtype.storm.tuple.Values;import java.util.Map;public class SplitSentenceBolt extends BaseRichBolt&#123; private OutputCollector collector; public void prepare(Map config, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; public void execute(Tuple tuple) &#123; String sentence = tuple.getStringByField(&quot;sentence&quot;); String[] words = sentence.split(&quot; &quot;); for(String word : words)&#123; this.collector.emit(new Values(word)); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;word&quot;)); &#125;&#125; WordCountBolt.java1234567891011121314151617181920212223242526272829303132333435import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Tuple;import backtype.storm.tuple.Values;import java.util.HashMap;import java.util.Map;public class WordCountBolt extends BaseRichBolt&#123; private OutputCollector collector; private HashMap&lt;String, Long&gt; counts = null; public void prepare(Map config, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; this.counts = new HashMap&lt;String, Long&gt;(); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(&quot;word&quot;); Long count = this.counts.get(word); if(count == null)&#123; count = 0L; &#125; count++; this.counts.put(word, count); this.collector.emit(new Values(word, count)); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;word&quot;, &quot;count&quot;)); &#125;&#125; ReportBolt.java1234567891011121314151617181920212223242526272829303132333435363738394041import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Tuple;import java.util.ArrayList;import java.util.Collections;import java.util.HashMap;import java.util.List;import java.util.Map;public class ReportBolt extends BaseRichBolt &#123; private HashMap&lt;String, Long&gt; counts = null; public void prepare(Map config, TopologyContext context, OutputCollector collector) &#123; this.counts = new HashMap&lt;String, Long&gt;(); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(&quot;word&quot;); Long count = tuple.getLongByField(&quot;count&quot;); this.counts.put(word, count); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // this bolt does not emit anything &#125; @Override public void cleanup() &#123; System.out.println(&quot;--- FINAL COUNTS ---&quot;); List&lt;String&gt; keys = new ArrayList&lt;String&gt;(); keys.addAll(this.counts.keySet()); Collections.sort(keys); for (String key : keys) &#123; System.out.println(key + &quot; : &quot; + this.counts.get(key)); &#125; System.out.println(&quot;--------------&quot;); &#125;&#125; Utils.java12345678910111213141516public class Utils &#123; public static void waitForSeconds(int seconds) &#123; try &#123; Thread.sleep(seconds * 1000); &#125; catch (InterruptedException e) &#123; &#125; &#125; public static void waitForMillis(long milliseconds) &#123; try &#123; Thread.sleep(milliseconds); &#125; catch (InterruptedException e) &#123; &#125; &#125;&#125; 转载请标明出处]]></content>
      <categories>
        <category>JStorm</category>
      </categories>
      <tags>
        <tag>JStorm</tag>
        <tag>Storm</tag>
        <tag>大数据</tag>
        <tag>流计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JStorm：任务调度]]></title>
    <url>%2F2017%2F09%2F10%2FJstormmanage%2F</url>
    <content type="text"><![CDATA[前一篇文章 JStorm：概念与编程模型 介绍了JStorm的基本概念以及编程模型方面的知识，本篇主要介绍自己对JStorm的任务调度方面的认识，主要从三个方面介绍： 调度角色 调度方法 自定义调度 调度角色上图是JStorm中一个topology对应的任务执行结构，其中worker是进程，executor对应于线程，task对应着spout或者bolt组件。 WorkerWorker是task的容器， 同一个worker只会执行同一个topology相关的task。 一个topology可能会在一个或者多个worker（工作进程）里面执行，每个worker执行整个topology的一部分。比如，对于并行度是300的topology来说，如果我们使用50个工作进程来执行，那么每个工作进程会处理其中的6个tasks。Storm会尽量均匀的工作分配给所有的worker。 ExecutorExecutor是在worker中的执行线程，在同一类executor中，要么全部是同一个bolt类的task，要么全部是同一个spout类的task，需要注意的是， 一个executor只能同时运行一个task，创建时将多个task设置在一个executor中，在前期Storm中主要考虑的是后期线程扩展（待验证），但是在JStorm中可以在rebalance时改变Task的数量，所以不需要将task数量大于executor。 TaskTask是真正任务的执行者，对应创建topology时建立的一个bolt或者spout组件。每一个spout和bolt会被当作很多task在整个集群里执行。可以调用TopologyBuilder类的setSpout和setBolt来设置并行度（也就是有多少个task）。 调度方法默认调度算法默认调度算法遵循以下的原则： 任务调度算法以worker为维度，尽量将平均分配到各个supervisor上； 以worker为单位，确认worker与task数目大致的对应关系(注意在这之前已经其他拓扑占用利用的worker不再参与本次动作)； 建立task-worker关系的优先级依次为：尽量避免同类task在同一work和supervisor下的情况，尽量保证task在worker和supervisor基准上平均分配，尽量保证有直接信息流传输的task在同一worker下。 调度过程中正在进行的调度动作不会对已发生的调度动作产生影响 调度示例如下是一个topology创建时配置代码，以及运行时的示意图。1234567891011//创建topology配置代码Config conf = new Config();conf.setNumWorkers(2); // use two worker processestopologyBuilder.setSpout(&quot;blue-spout&quot;, new BlueSpout(), 2);topologyBuilder.setBolt(&quot;green-bolt&quot;, new GreenBolt(), 2) .setNumTasks(4) .shuffleGrouping(&quot;blue-spout&quot;);topologyBuilder.setBolt(&quot;yellow-bolt&quot;, new YellowBolt(), 6) .shuffleGrouping(&quot;green-bolt&quot;);StormSubmitter.submitTopology(&quot;mytopology&quot;, conf, topologyBuilder.createTopology()); 参考以上代码，以及任务调度算法，该拓扑中，设为worker为2，蓝色Spout并发设置为2，task默认与并发相同为2；绿色Bolt执行并发为2，但设置其task为4，所以每个executor中有两个Task，黄色Bolt并发为6，task默认与并发相同为6。图中两个worker是一致的，可以认为是JStorm分配任务时做的权衡，尽量分配的均匀，不代表所有情况都是如此。 分发过程上图是storm的示例，JStorm雷同。JStorm任务分发过程： 客户端提交拓扑到nimbus，并开始执行； Nimbus针对该拓扑建立本地的目录，根据topology的配置计算task，分配task，在zookeeper上建立assignments节点存储task和supervisor机器节点中woker的对应关系； 在zookeeper上创建taskbeats节点来监控task的心跳；启动topology。 各Supervisor去zookeeper上获取分配的tasks，启动多个woker进行，每个woker生成task；根据topology信息初始化建立task之间的连接。 自定义调度JStorm支持一下自定义调度设置： 设置每个worker的默认内存大小 1ConfigExtension.setMemSizePerWorker(Map conf, long memSize) 设置每个worker的cgroup,cpu权重 1ConfigExtension.setCpuSlotNumPerWorker(Map conf, int slotNum) 设置是否使用旧的分配方式 1ConfigExtension.setUseOldAssignment(Map conf, boolean useOld) 设置强制某个component的task 运行在不同的节点上 1ConfigExtension.setTaskOnDifferentNode(Map componentConf, boolean isIsolate) 注意，这个配置componentConf是component的配置， 需要执行addConfigurations 加入到spout或bolt的configuration当中 自定义worker分配1234567WorkerAssignment worker = new WorkerAssignment();worker.addComponent(String compenentName, Integer num);//在这个worker上增加一个taskworker.setHostName(String hostName);//强制这个worker在某台机器上worker.setJvm(String jvm);//设置这个worker的jvm参数worker.setMem(long mem); //设置这个worker的内存大小worker.setCpu(int slotNum); //设置cpu的权重大小ConfigExtension.setUserDefineAssignment(Map conf, List&lt;WorkerAssignment&gt; userDefines) 注:每一个worker的参数并不需要被全部设置,worker属性在合法的前提下即使只设置了部分参数也仍会生效 强制topology运行在一些supervisor上在实际应用中， 常常一些机器部署了本地服务（比如本地DB）， 为了提高性能， 让这个topology的所有task强制运行在这些机器上1conf.put(Config.ISOLATION_SCHEDULER_MACHINES, List&lt;String&gt; isolationHosts) conf 是topology的configuration 转载请标明出处]]></content>
      <categories>
        <category>JStorm</category>
      </categories>
      <tags>
        <tag>JStorm</tag>
        <tag>Storm</tag>
        <tag>大数据</tag>
        <tag>流计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JStorm：概念与编程模型]]></title>
    <url>%2F2017%2F08%2F25%2FJStorm1%2F</url>
    <content type="text"><![CDATA[1、集群架构JStorm从设计的角度，就是一个典型的调度系统，简单集群的架构如下图所示，其中Nimbus可增加一个备节点，多个Supervisor节点组成任务执行集群。 1.1、NimbusNimbus是作为整个集群的调度器角色，负责分发topology代码、分配任务，监控集群运行状态等，其主要通过ZK与supervisor交互。可以和Supervisor运行在同一物理机上，JStorm中Nimbus可采用主从备份，支持热切。 1.2、SupervisorSupervisor 是集群中任务的执行者，负责运行具体任务以及关闭任务。其从ZK中监听nimbus的指令，然后接收分发代码和任务并执行、监控反馈任务执行情况。 1.3 、ZookeeperZK是整个系统中的协调者，Nimbus的任务调度通过ZK下发至Supervisor来执行。 2、Topology编程模型Topology是一个可以在JStorm中运行的任务的抽象表达，在JStorm的topology中，有两种组件：spout和bolt。下面是一张比较经典的Topology结构图。每一个topology，既可以有多个spout，代表同时从多个数据源接收消息，也可以多个bolt，来执行不同的业务逻辑。一个topology会一直运行直到你手动kill掉，JStorm自动重新分配执行失败的任务。在JStorm中有对于流stream的抽象，流是一个不间断的无界的连续tuple，注意JStorm在建模事件流时，把流中的事件抽象为tuple即元组。我们可以认为spout就是一个一个的水龙头，并且每个水龙头里流出的水是不同的tuple，我们想拿到哪种水tuple就拧开哪个水龙头，然后使用管道将水龙头的水tuple导向到一个水处理器（bolt），水处理器bolt处理后再使用管道导向另一个处理器或者存入容器中。JStorm将上图抽象为Topology即拓扑，拓扑结构是有向无环的，拓扑是Jstorm中最高层次的一个抽象概念，它可以被提交到Jstorm集群执行，一个拓扑就是一个数据流转换图，图中每个节点是一个spout或者bolt，图中的边表示bolt订阅了哪些流，当spout或者bolt发送元组到流时，它就发送元组到每个订阅了该流的bolt。 2.1、spoutJStorm认为每个stream都有一个stream源，也就是原始元组的源头，所以它将这个源头抽象为spout，spout可能是连接消息中间件（如MetaQ， Kafka， TBNotify等），并不断发出消息，也可能是从某个队列中不断读取队列元素并装配为tuple发射。JStorm框架对spout组件定义了一个主要方法：nextTuple，顾名思义，就是获取下一条消息。执行时，可以理解成JStorm框架会不停地调这个接口，以从数据源拉取数据并往bolt发送数据。Tuple是一次消息传递的基本单元，tuple里的每个字段一个名字,并且不同tuple的对应字段的类型必须一样。tuple的字段类型可以是： integer, long, short, byte, string, double, float, boolean和byte array。还可以自定义类型，只要实现对应的序列化器。JStorm中与spout相关的接口主要是ISpout和IRichSpout、IBatchSpout，后两接口实现了对ISpout接口的上层封装。 ISpout接口主要方法：open：在worker中初始化该ISpout时调用，一般用来设置一些属性：比如从spring容器中获取对应的Bean。close：和open相对应（在要关闭的时候调用）。activate：从非活动状态变为活动状态时调用。deactivate：和activate相对应（从活动状态变为非活动状态时调用）。nextTuple：JStorm希望在每次调用该方法的时候，它会通过collector.emit发射一个tuple。ack：jstorm发现msgId对应的tuple被成功地完整消费会调用该方法。fail：和ack相对应（jstorm发现某个tuple在某个环节失败了）。和ack一起保证tuple一定被处理。 2.2、boltJStorm将tuple的中间处理过程抽象为Bolt，bolt可以消费任意数量的输入流，只要将流方向导向该bolt，同时它也可以发送新的流给其他bolt使用，这样一来，只要打开特定的spout（管口）再将spout中流出的tuple导向特定的bolt，然后bolt对导入的流做处理后再导向其他bolt或者目的地。bolt代表处理逻辑，bolt收到消息之后，对消息做处理（即执行用户的业务逻辑），处理完以后，既可以将处理后的消息继续发送到下游的bolt，这样会形成一个处理流水线（不过更复杂的情况应该是个有向图）；也可以直接结束。bolt组件主要方法：execute，这个接口就是用户用来处理业务逻辑的地方。通常一个流水线的最后一个bolt，会做一些数据的存储工作，比如将实时计算出来的数据写入DB、HBase等，以供前台业务进行查询和展现。Bolts可以发射多条消息流， 使用OutputFieldsDeclarer.declareStream定义stream，使用OutputCollector.emit来选择要发射的stream。在保证不丢消息的场景中，在bolts必须要为它处理的每一个tuple调用OutputCollector的ack方法，以通知JStorm这个tuple被处理完成了，从而通知这个tuple的发射者spouts。 一般的流程是： bolts处理一个输入tuple, 发射0个或者多个tuple, 然后调用ack通知JStorm自己已经处理过这个tuple了。JStorm提供了一个IBasicBolt会自动调用ack。JStorm中与Bolt相关的接口主要是IBolt，IRichBolt，IBasicBolt和IBatchBolt，后面接口实现了对IBolt接口的上层封装。 IBolt接口的主要方法：prepare：在worker中初始化该IBolt时调用，一般用来设置一些属性：比如从spring容器中获取对应的Bean。cleanup：和prepare相对应（在显示关闭topology的时候调用）execute：处理jstorm发送过来的tuple。 2.3、TupleJStorm将流中数据抽象为tuple，一个tuple就是一个值列表value list，list中的每个value都有一个name，tuple可以由任意类型组合而成，因为storm是分布式的，所以它需要知道在task间如何序列化和反序列化数据的。storm使用Kryo进行序列化，Kryo是java开发中一个快速灵活序列器。默认情况下，storm可以序列化基础类型，比如字符串，字节，数组，ArrayList, HashMap, HashSet和 Clojure 集合类型，如果需要使用其他类型，需要自定义序列器。拓扑的每个节点都要说明它所发射出的元组的字段的name，其他节点只需要订阅该name就可以接收处理。在spout和Bolt组件中，使用declareOutputFields方法定义发射出的tuple的字段名。 3、小结本文主要讲述了JStorm中集群的架构以及Topology编程模型方面的概念知识，后续会更深入的写一些实践、运维、原理等方面的文章。]]></content>
      <categories>
        <category>JStorm</category>
      </categories>
      <tags>
        <tag>JStorm</tag>
        <tag>Storm</tag>
        <tag>大数据</tag>
        <tag>流计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提高Elasticsearch性能的配置建议]]></title>
    <url>%2F2017%2F03%2F18%2Felasticsearch%2F</url>
    <content type="text"><![CDATA[之前公司项目中有使用Elasticsearch存储日志，当时使用的功能简单，并没有深入了解Elasticsearch，但是对于该支持文本搜索的存储架构还是很感兴趣，最近因为想在一个新项目中采用ELK（Elasticsearch+Logstash+Kibana）技术栈来存储系统日志，学习有关Elasticsearch的书籍（深入理解Elasticsearch，第二版），现在就书本的第八章——提高性能，总结一些有关使用Elasticsearch的Tips，该书采用的elasticsearch为1.4.X版本。 热点线程检测热点线程API能向你提供系统变慢变卡顿的必需信息，它给出了什么可能是热点的信息，并使你可以看到系统的哪部分需要更深入的分析，例如查询的执行或者Lucene段的合并。热点线程API返回从CPU的角度来看，elasticsearch哪部分的代码可能是热点的信息，或者由于某些原因elasticsearch卡在了哪里。 使用方法通过使用如下的命令你可以查看所有节点或者某些、某个节点的情况。 12/_nodes/hot_threads/_nodes/&#123;node or nodes&#125;/hot_threads 例如为了查看所有节点上的热点线程，你可以执行如下的命令：1curl &apos;localhost:9200/_nodes/hot_threads&apos; 此API支持的参数包括 threads 需要分析的线程数，默认3 interval 前后两次检查的时间间隔 type 需要检查的线程状态的类型，默认是CPU，可以是阻塞、等待等线程状态 snapshots 需要生产堆栈跟踪快照的数量 例如，想要以1s为周期查看所有节点上处于等待状态的热点线程，可以执行命令：1curl &apos;localhost:9200/_nodes/hot_threads?type=wait&amp;interval=1s&apos; 执行原理热点线程检测执行流程如下： elasticsearch选取所有运行的线程，收集线程花费CPU的各种信息。 等待interval参数指定的时间后，再次收集步骤1中同样的信息。 对线程基于其消耗的时间进行排序，取前N（参数threads决定）线程分析 每隔几毫秒，对3中选择的线程获取一些堆栈的快照，（数量由snapshots参数决定） 组合堆栈信息，返回响应 返回的响应包括：线程所属节点、消耗CPU时间的百分比、使用CPU的方式、线程名，最后跟着一个堆栈跟踪信息，通过以上信息可以定位节点的性能问题。 高负载场景的分类高负载场景可以分为三种情况： 专注于高索引负载 专注于高查询负载 高查询索引负载并行 后面按照三个场景进行说明 查询、索引负载均衡场景因为是查询、索引负载均衡的场景，所以一下建议不只是与索引性能、查询性能有关，而是与它们都有关。 正确的存储选择正确的存储实现，在运行1.3版本以后时尤其重要。 如果使用64位系统，考虑使用 mmapfs（内存映射） 基于UNIX系统考虑选择 Niofs windows系统应该选择 simplefs 非持久化的存储考虑 内存存储 elasticsearch 1.3版本以后，默认使用的存储类型是一个 混合 的存储类型default，使用内存映射文件读取term字典，doc values，其他文件采用nio存储。 索引刷新频率索引刷新的频率是指文档需要多长时间才能出现在搜索结果中。规则非常简单：刷新频率越短，查询越慢，且索引文档的吞吐量越低。默认的刷新频率是1s，这意味着索引查询器每1s都会重新打开一次。如果可以接受一个较慢的刷新频率，可以设置成5s、10s、30s等。 线程池调优但你看到节点正在填充队列并且仍然有计算能力剩余，且这些计算能力可以被指定用于处理等待处理操作。线程池调优包括线程数以及等待队列长度两个方面。 调整合并过程Lucene段合并取决与你追加多少数据、多久追加一次等因素，对于Lucene分段和合并，需要记住：有多个段的索引执行查询比只有少量段的索引上执行慢。性能测试显示多个段上执行查询比只有一个段的索引要慢大约10%-15%。 如果你希望查询快，就需要更少的段 段合并限流，默认情况下elasticsearch会限制合并的速度在20MB/s，elasticsearch需要限流来避免合并过程中过多的影响搜索。如果使用的是SSD硬盘，那么默认20MB/s是不适合的，通过一下参数设置： indices.store.throttle.max_bytes_per_sec设置端合并限流 高查询频率场景缓存设置第一个有助于查询性能的缓存是 过滤器缓存 ，可以使用下面的属性，来控制给定节点上能够被过滤器缓存使用的全部内存数量，默认是10%。1indices.cache.filter.size 第二个缓存是 分片查询缓存 ，她的目的是缓存聚合、提示词结果、命中数等，当你的查询使用了聚合、提示词等，最好启用这个缓存。该缓存的大小可以使用如下参数设置：1indices.cache.query.size 查询的思考 总是考虑到优化查询结构、过滤器使用等 过滤器不影响文档的打分，在计算得分时不被考虑进去 使用路由如果数据可以使用路由，你应该考虑使用它，可以避免在请求特定数据查询时查询所有的分片。 控制size和shard_size在处理聚合查询时，合理的设置size、shard_size，size定义了聚合结果返回多少组数据，聚合只会返回前size个结果给客户端；size、shard_size具有相同的意思，只是shard_size其作用是在分片的层次上。 高索引吞吐场景批量索引合理的使用批量索引可以显著提高索引的速度，但不要向elasticsearch发送过多的超出其能力的批量索引请求。 doc value 与索引速度的权衡doc value可以帮助具有 排序、聚合、分组 的操作，但是记录doc value需要在索引时做一些额外的操作，这样会 降低索引速度 和 索引吞吐量 ，所以需要结合具体应用场景，权衡 doc value与索引速度。 控制文档的字段尽量的保持你存储的字段尽可能的少，你在打多少情况下需要保存的字段是_source，在一些场景下需要判断是否需要存储 _all、_source 等字段。在禁用_all字段时，设置一个新的默认搜索字段是一个很好的实践，使用如下命令设置：1index.query.default_field：set_your_name 调整事务日志elasticsearch使用事务日志来获取最新的更新，确保数据的持久化以及优化Lucene索引的写入，默认的事务日志最多保留5000个操作，或者最多占用200MB的空间。两者的参数设置如下：12index.translog.flush_threshold_opsindex.translog.flush_threshold_size 如果需要获取更大的索引吞吐量，愿意付出数据在更长的时间内不能被搜索到，可以调高以上两个默认值。并且在故障发生时，拥有大量事务日志的节点需要更长时间去恢复。 最后以上是有关elasticsearch使用中的小Tips，后期有时间会继续写一些有关elk技术栈的文章。 转载请标明出处]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>Java</tag>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客搭建与配置：Hexo-github-域名绑定]]></title>
    <url>%2F2016%2F10%2F11%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%9AHexo-github-%E5%9F%9F%E5%90%8D%E7%BB%91%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[最近想搭建个人的博客网站，查询一些信息以后决定采用Hexo+Github Pages的方式。该方法站在巨人的肩膀上，很是方便快捷，找了个很喜欢的主题，做了点配置就可以，最后在阿里云上买了个域名，成功搞定就差写博客了。这里就说一下建站的整个过程。个人博客地址http://he-zhao.cn 前期准备 Github账号注册 Node.js安装 git安装 Github Pages注册了Github账号以后，每个账号可以建一个Github Pages Create a new repository后，严格使用你的github用户名+.github.io命名新的repository，例如我的用户名为mrgiser，新建的取名为 mrgiser.github.io，其他设置不用关心，这样GIthub Pages 所需要的版本库也创建好了。 安装Hexo安装好Git跟Node.js后，在cmd中执行12$ npm install -g hexo-cli //安装hexo客户端$ npm install hexo-deployer-git --save //安装用于部署到Git的插件 以上完成之后，执行下面的命令，Hexo将会在指定文件夹中新建所需要的文件。123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 至此，hexo安装完成。 修改主题其实安装hexo的过程很简单，主要时间花在了找主题以及主题配置上了。我选择了一款在NexT上修改的主题iissnan。 进入Hexo文件夹。然后使用下面的命令clone下iissnan主题，主题的文件代码将被下载至themes/iissnan文件夹中。1$ git clone https://github.com/mrgiser/mrgiser.github.io.git themes/iissnan 打开博客主文件夹下的_config.yml，修改其中的theme 属性。theme: 后面要加空格。1theme: iissnan 本地部署部署在本地方便查看调试效果，命令如下：12$ hexo g #在public文件夹下生成静态页面$ hexo s #启动本地服务，进行文章预览调试,浏览器输入http://localhost:4000查看效果 发布到Github Pages先对Git进行配置：12$ git config --global user.name &quot;your name&quot;$ git config --global user.email &quot;email@email.com&quot; 博客主文件夹下的_config.yml,也就是 站点配置文件 ，配置其中的deploy参数，详细请查看官方文档中部署部分。我的设置如下所示：1234deploy: type: git repository: git@github.com:mrgiser/mrgiser.github.io.git branch: master 配置完成保存后，执行以下命令将代码同步到github pages上：12$ hexo g #需要先生成静态文件$ hexo d #将代码部署到github 输入https://XXXX.github.io访问个人github pages，其中XXXX为你github的用户名。 Hexo配置请记住博客主文件夹下的blog_config.yml为站点配置文件，主题配置文件为blog\themes\iissnan_config.yml。 作者、标题、描述、语言等站点配置修改 站点配置文件 中的如下配置：1234567# Sitetitle: Pegasussubtitle: description: 记录生活，写点东西author: Pegasus.Helanguage: zh-Hanstimezone: 导航栏与侧边栏在导航栏中加入归档、分类、标签、关于等，在主题配置文件中修改如下部分123456menu: home: / || home archives: /archives/ || archive tags: /tags/ || tags categories: /categories/ || th about: /about/ || user 社交信息添加在主题配置文件中修改如下部分123456social: GitHub: https://github.com/XXXXXX|| github E-Mail: mailto:XXXXXXXX || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook 头像在主题配置文件中修改如下部分,添加头像照片1234# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: /images/IMG_3428.JPG 其他主题配置其他主题配置可参考next使用文档，Git上的有关主题的问答 域名配置这里讲述在阿里云上购买的域名如何配置，登陆阿里云-控制台-域名服务-域名，选择需要配置的域名，点击解析。删除默认所有主机记录为@的记录，添加解析参考如下： 在blog\source\文件夹下新建文件CNAME（在此新建文件，可以保证hexo d的时候不会删除掉），文本打开编辑，添加个人购买的域名，例如he-zhao.cn重新部署12$ hexo g #需要先生成静态文件$ hexo d #将代码部署到github 至此域名绑定就完成了，访问自己的域名时，会显示github pages的页面。 写在最后个人博客网站建好了，更重要的是好好记录。最后个人博客地址希望支持，后期会增加一些技术博客，希望支持。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Java读书笔记]]></title>
    <url>%2F2016%2F08%2F13%2FEffectiveJava%2F</url>
    <content type="text"><![CDATA[创建和销毁对象考虑使用静态工厂代替构造函数 静态工厂具有名称，可读性抢 不必在每次调用时都创建新对象，单例模式 可以返回原类型的子类型对象 在创建类似List Map时，代码简洁 缺点：类如果不含有公有或者受保护的构造器，就不能被子类化；其与静态函数没有区别。 遇到多个构造函数参数时，考虑使用构建器 参数多时并可选，构造函数很多，静态工厂也是如此 构建器，set方法返回this，调用可以连接起来。 使用私有构造器或者枚举类型强化单例属性 使用反射机制，可以调用私有构造器 序列化时，提供readResolve方法保证单一 单元素的枚举类型成为实现单例的最佳方法 避免创建不必要的对象 不需要new String，重用不可变的String对象 注意基本类型与装箱类型，优先使用基本类型，装箱类型每次都会新建。 只有在对象非常重量级时，才考虑使用对象池 消除过期的对象引用 如果一个栈先增长，再收缩；从栈中弹出来的对象将不会被当做垃圾回收。Object[] 内存泄露常见来源：缓存、监听器、其他回调 避免使用终结方法 finalizer通常是不可预测的，也是很危险的，一般情况不适用 中介方法的缺点在于不能保证会被及时的执行 java不保证终结方法会被及时的执行，而且根本不保证他们会被执行 异常发生在终结方法中，警告都不会打出 终结方法有一个非常严重的性能损耗 使用终结方法，记得使用super.finalizer 使用try finally代替 所有对象都通用的方法覆盖equals时，遵守通用约定 类具有自己特有的逻辑相等概念时，覆盖equals 需要满足自反性、对称性、传递性、一致性，null必不相等 覆盖equals时必须覆盖hashCode 不要企图让equals过于智能，加上@Override 覆盖Equals时要覆盖hashCode equals所用信息不变，code多次执行结果不变 equals相等，code相同 equals不同，code可能相同，散列的分散性 不要试图从散列码中排除一个关键部分来提高性能 散列码缓存在对象内部 延迟初始化散列码 始终要覆盖toString toString应该包含对象中包含的所有值得关注的信息 谨慎地覆盖clone final与clone冲突 对于一个为了继承而设计的类，如果未能提供行为良好的受保护的clone方法，它的子类就不可能实现cloneable接口 考虑实现Comparable接口 对象小于、等于、大于指定对象时，返回负数、0、正数 自反性、对称性、传递性 类和接口使类和成员的可访问性最小化 信息隐藏与封装是软件设计的原则 有效的解除组成系统的个模块之间的耦合关系 尽可能的使每个类或成员不被外界访问 私有-&gt;包级私有-&gt;收保护-&gt;公有 受保护的成员应该尽少使用，是对外API的一部分，需要维护 长度非零的数组总是可变，类具有公有的静态final数组，或者返回这域的访问方法，几乎总是错误的 在公有类中使用set、get而不是公有域使可变性最小化 不可变对象比较简单 线程安全、不要求同步 缺点：不同的值都需要一个单独的对象 复合优先与继承 在包内继承，是非常安全的 进行夸包边界的继承，是非常危险的 继承打破了封装性 只有子类真正是超累的子类型时 is-a，才试用继承 接口优与抽象类 java单继承，抽象类受到极大的限制 函数指针实现策略模式 Arrays.sort优先使用静态成员类 静态成员类、非静态成员类、匿名类、局部类 如果成员类不需要访问外围类实例，就使用静态成员类]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
</search>
